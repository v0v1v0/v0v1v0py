<div class="container">

<table style="width: 100%;"><tr>
<td>spec</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Specificity</h2>

<h3>Description</h3>

<p>These functions calculate the <code>spec()</code> (specificity) of a measurement system
compared to a reference result (the "truth" or gold standard).
Highly related functions are <code>sens()</code>, <code>ppv()</code>, and <code>npv()</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">spec(data, ...)

## S3 method for class 'data.frame'
spec(
  data,
  truth,
  estimate,
  estimator = NULL,
  na_rm = TRUE,
  case_weights = NULL,
  event_level = yardstick_event_level(),
  ...
)

spec_vec(
  truth,
  estimate,
  estimator = NULL,
  na_rm = TRUE,
  case_weights = NULL,
  event_level = yardstick_event_level(),
  ...
)

specificity(data, ...)

## S3 method for class 'data.frame'
specificity(
  data,
  truth,
  estimate,
  estimator = NULL,
  na_rm = TRUE,
  case_weights = NULL,
  event_level = yardstick_event_level(),
  ...
)

specificity_vec(
  truth,
  estimate,
  estimator = NULL,
  na_rm = TRUE,
  case_weights = NULL,
  event_level = yardstick_event_level(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>Either a <code>data.frame</code> containing the columns specified by the
<code>truth</code> and <code>estimate</code> arguments, or a <code>table</code>/<code>matrix</code> where the true
class results should be in the columns of the table.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Not currently used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>truth</code></td>
<td>
<p>The column identifier for the true class results
(that is a <code>factor</code>). This should be an unquoted column name although
this argument is passed by expression and supports
quasiquotation (you can unquote column
names). For <code style="white-space: pre;">⁠_vec()⁠</code> functions, a <code>factor</code> vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>estimate</code></td>
<td>
<p>The column identifier for the predicted class
results (that is also <code>factor</code>). As with <code>truth</code> this can be
specified different ways but the primary method is to use an
unquoted variable name. For <code style="white-space: pre;">⁠_vec()⁠</code> functions, a <code>factor</code> vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>estimator</code></td>
<td>
<p>One of: <code>"binary"</code>, <code>"macro"</code>, <code>"macro_weighted"</code>,
or <code>"micro"</code> to specify the type of averaging to be done. <code>"binary"</code> is
only relevant for the two class case. The other three are general methods
for calculating multiclass metrics. The default will automatically choose
<code>"binary"</code> or <code>"macro"</code> based on <code>estimate</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>na_rm</code></td>
<td>
<p>A <code>logical</code> value indicating whether <code>NA</code>
values should be stripped before the computation proceeds.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>case_weights</code></td>
<td>
<p>The optional column identifier for case weights.
This should be an unquoted column name that evaluates to a numeric column
in <code>data</code>. For <code style="white-space: pre;">⁠_vec()⁠</code> functions, a numeric vector,
<code>hardhat::importance_weights()</code>, or <code>hardhat::frequency_weights()</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>event_level</code></td>
<td>
<p>A single string. Either <code>"first"</code> or <code>"second"</code> to specify
which level of <code>truth</code> to consider as the "event". This argument is only
applicable when <code>estimator = "binary"</code>. The default uses an internal helper
that defaults to <code>"first"</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The specificity measures the proportion of negatives that are correctly
identified as negatives.
</p>
<p>When the denominator of the calculation is <code>0</code>, specificity is undefined.
This happens when both <code style="white-space: pre;">⁠# true_negative = 0⁠</code> and <code style="white-space: pre;">⁠# false_positive = 0⁠</code>
are true, which mean that there were no true negatives. When computing binary
specificity, a <code>NA</code> value will be returned with a warning. When computing
multiclass specificity, the individual <code>NA</code> values will be removed, and the
computation will procede, with a warning.
</p>


<h3>Value</h3>

<p>A <code>tibble</code> with columns <code>.metric</code>, <code>.estimator</code>,
and <code>.estimate</code> and 1 row of values.
</p>
<p>For grouped data frames, the number of rows returned will be the same as
the number of groups.
</p>
<p>For <code>spec_vec()</code>, a single <code>numeric</code> value (or <code>NA</code>).
</p>


<h3>Relevant Level</h3>

<p>There is no common convention on which factor level should
automatically be considered the "event" or "positive" result
when computing binary classification metrics. In <code>yardstick</code>, the default
is to use the <em>first</em> level. To alter this, change the argument
<code>event_level</code> to <code>"second"</code> to consider the <em>last</em> level of the factor the
level of interest. For multiclass extensions involving one-vs-all
comparisons (such as macro averaging), this option is ignored and
the "one" level is always the relevant result.
</p>


<h3>Multiclass</h3>

<p>Macro, micro, and macro-weighted averaging is available for this metric.
The default is to select macro averaging if a <code>truth</code> factor with more
than 2 levels is provided. Otherwise, a standard binary calculation is done.
See <code>vignette("multiclass", "yardstick")</code> for more information.
</p>


<h3>Implementation</h3>

<p>Suppose a 2x2 table with notation:
</p>

<table>
<tr>
<td style="text-align: right;"> </td>
<td style="text-align: center;"> Reference </td>
<td style="text-align: center;"> </td>
</tr>
<tr>
<td style="text-align: right;"> Predicted </td>
<td style="text-align: center;"> Positive </td>
<td style="text-align: center;"> Negative
</td>
</tr>
<tr>
<td style="text-align: right;"> Positive </td>
<td style="text-align: center;"> A </td>
<td style="text-align: center;"> B </td>
</tr>
<tr>
<td style="text-align: right;"> Negative </td>
<td style="text-align: center;"> C </td>
<td style="text-align: center;"> D </td>
</tr>
<tr>
<td style="text-align: right;"> </td>
</tr>
</table>
<p>The formulas used here are:
</p>
<p style="text-align: center;"><code class="reqn">Sensitivity = A/(A+C)</code>
</p>

<p style="text-align: center;"><code class="reqn">Specificity = D/(B+D)</code>
</p>

<p style="text-align: center;"><code class="reqn">Prevalence = (A+C)/(A+B+C+D)</code>
</p>

<p style="text-align: center;"><code class="reqn">PPV = (Sensitivity * Prevalence) / ((Sensitivity * Prevalence) + ((1-Specificity) * (1-Prevalence)))</code>
</p>

<p style="text-align: center;"><code class="reqn">NPV = (Specificity * (1-Prevalence)) / (((1-Sensitivity) * Prevalence) + ((Specificity) * (1-Prevalence)))</code>
</p>

<p>See the references for discussions of the statistics.
</p>


<h3>Author(s)</h3>

<p>Max Kuhn
</p>


<h3>References</h3>

<p>Altman, D.G., Bland, J.M. (1994) “Diagnostic tests 1:
sensitivity and specificity,” <em>British Medical Journal</em>,
vol 308, 1552.
</p>


<h3>See Also</h3>

<p>Other class metrics: 
<code>accuracy()</code>,
<code>bal_accuracy()</code>,
<code>detection_prevalence()</code>,
<code>f_meas()</code>,
<code>j_index()</code>,
<code>kap()</code>,
<code>mcc()</code>,
<code>npv()</code>,
<code>ppv()</code>,
<code>precision()</code>,
<code>recall()</code>,
<code>sens()</code>
</p>
<p>Other sensitivity metrics: 
<code>npv()</code>,
<code>ppv()</code>,
<code>sens()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Two class
data("two_class_example")
spec(two_class_example, truth, predicted)

# Multiclass
library(dplyr)
data(hpc_cv)

hpc_cv %&gt;%
  filter(Resample == "Fold01") %&gt;%
  spec(obs, pred)

# Groups are respected
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  spec(obs, pred)

# Weighted macro averaging
hpc_cv %&gt;%
  group_by(Resample) %&gt;%
  spec(obs, pred, estimator = "macro_weighted")

# Vector version
spec_vec(
  two_class_example$truth,
  two_class_example$predicted
)

# Making Class2 the "relevant" level
spec_vec(
  two_class_example$truth,
  two_class_example$predicted,
  event_level = "second"
)
</code></pre>


</div>